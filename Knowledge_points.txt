MCCFR
1.  先从当前的信息集合里找出每个选择的遗憾值
2.  然后在所有遗憾值中按概率选一个操作

3.  做完此操作后计算效益
4.  如果是正效益，则加此操作的遗憾值， 减少别的操作的遗憾值。负效益的话则相反。


CFR
1. train
1. 先获取所有可能的行动。
2. 然后在policy（遺憾值匹配的策略）中找到每个行动的概率
3. 计算所有行动的效益
4. 根据效益计算遗憾值,当前行动遗憾值=当前行动效益减去所有行动效益。
二. update policy
1. 归一化遗憾值
2. 根据遗憾值更新策略，更新当前信息集合下每个行动的概率


DeepCFR

简单描述：
1. 有两种网络，第一种是advantage网络  每次运行完几轮要重新初始化的。另一种网络是policy网络，是最终要得到的网络。
2. 先说advantage网络。 在当轮训练一个网络，然后在下一轮用这个网络计算regrets值。 网络的意义：如果不用这个网络的话，只用policy网络可能会陷入单个action的死循环中，（猜测，论文里没写）。
3. policy网络就是训练所有样本。

后面修改运行步骤：
1. 初始化
2. 在当前初始化中循环N次， 每次循环更新regret值（1. regrets值是用上一次训练的advantage网络以及当前round的payoff计算得到的。2. 这个每次循环相当于采样N次）。然后把regret保存下来
3. 使用上次N次循环得到的训练数据训练一遍advantage网络。
4. 循环执行步骤2和3 保存样本
5. 用保存的样本训练policy网络


注意 ： 之后要修改N。把N修改大一点  这样每次训练出来的advantage网络会好一些，可能会收敛更快。